<h1>Legislative Regulation</h1>
<hr />
<p>Many countries have implemented legislative regulations in recent years to combat widespread abuse attacks, including misinformation and hate speech campaigns. Let us briefly explore two approaches to these regulations implemented in the <code>United States (US)</code> and the <code>European Union (EU)</code>.</p>
<p>One of the main difficulties with legislative regulations regarding LLMs is balancing accountability and innovation. As LLMs provide benefits in many areas, such as education, accessibility, and creativity, it is important not to restrict their abilities. However, their capacity for generating harmful content cannot be ignored. On top of that, the definition of accountability and liability is crucial. After all, who is responsible for LLM-generated content? Model developers, model deployers, or end-users providing an input prompt? This is one of the core issues legislative regulations need to address.</p>
<p>Furthermore, while combating abuse attacks is essential, it is important not to impede fundamental human rights such as <code>freedom of speech</code>. Thus, legislative regulations must focus on protecting against abuse attacks while not enabling overly tight restrictions or outright censorship.</p>
<hr />
<h2>Regulation in the US</h2>
<p>In the US, spreading misinformation, unless it crosses into defamation, incitement to violence, or fraud, is generally protected speech. Consequently, regulatory measures are challenging to implement. However, there are policies that aim to combat AI abuse attacks, such as the <a href="https://en.wikipedia.org/wiki/TAKE_IT_DOWN_Act">Take It Down Act</a>. The <code>Take It Down Act</code> targets abuse attacks in the form of deepfakes, which are AI-manipulated images, videos, or audio clips, by criminalizing the spreading of certain types of abusive material, such as <code>non-consensual intimate imagery</code>. In particular, the act explicitly includes AI-generated abusive material. Criminalizing the publication of such material aims to mitigate this specific type of targeted abuse attack.</p>
<p>Moreover, there are voluntary best practices that service providers can abide by, such as NIST's <a href="https://www.nist.gov/itl/ai-risk-management-framework">AI Risk Management Framework (AI RMF)</a>. The AI RMF details how trustworthy AI systems can look, their attributes, and how to develop and deploy such systems. By implementing these best practices, model developers and model deployers can mitigate the risk of the AI being used in abuse attacks. Moreover, agencies like the <code>Federal Trade Commission (FTC)</code> have explored ways to regulate deceptive practices by companies deploying AI. If LLMs are used in fraud or misleading commercial practices, the FTC can intervene to <a href="https://www.ftc.gov/news-events/news/press-releases/2024/09/ftc-announces-crackdown-deceptive-ai-claims-schemes">protect consumers</a>.</p>
<hr />
<h2>Regulation in the EU</h2>
<p>EU regulations consist of two core acts: the <a href="https://commission.europa.eu/strategy-and-policy/priorities-2019-2024/europe-fit-digital-age/digital-services-act_en">Digital Services Act (DSA)</a> and the <a href="https://artificialintelligenceact.eu/">EU Artificial Intelligence Act (AI Act)</a>.</p>
<p>The <code>DSA</code> requires digital service providers to implement mechanisms for <code>reporting and removing illegal content</code>. These mechanisms need to include systems that enable users to report illegal material. The service providers are required to react to such reports accordingly. Additionally, the DSA requires an <code>appeal system</code> for users whose content was falsely removed. It applies to all digital service providers offering a service to recipients in the EU, regardless of their location. More specifically, it applies even to service providers outside of the EU if they offer a digital service to users inside the EU. The DSA applies to all kinds of illegal content, i.e., it is not limited to AI-generated content.</p>
<p>For certain digital service providers, the DSA goes even further and mandates recurring <code>risk assessments</code> of the digital services. These risk assessments have to include issues like misinformation and cyber violence. Based on the results of these assessments, digital service providers need to implement <code>effective mitigations</code>, such as modifications to recommendation algorithms to prevent the spreading of harmful content, or increase moderation efforts. Furthermore, the DSA requires these digital service providers to disclose information about their content moderation policies, algorithmic systems, and ad targeting practices. Through these measures, the DSA is able to:</p>
<ul>
<li>Protect users from harmful content.</li>
<li>Protect users' fundamental rights, such as privacy or freedom of expression.</li>
<li>Provide mechanisms for reporting and removing illegal content, including harassment.</li>
<li>Promote transparency.</li>
<li>Strengthen accountability through risk assessments and mitigations.</li>
</ul>
<p>Complementary to the DSA, the <code>AI Act</code> is a complex legal framework that applies specifically to AI, such as LLMs. It aims to ensure the safe and ethical use of AI, applying to providers and deployers of AI applications providing services in the EU. Under the AI Act, AI applications are rated by their level of risk and defines different obligations depending on the level of risk:</p>
<ul>
<li>
<code>Unacceptable-risk AI systems</code>: These include <code>social scoring</code> systems or AI systems that cause significant harm by employing manipulative techniques, impairing informed decision-making, or exploiting vulnerabilities. These systems are <code>banned</code>.</li>
<li>
<code>High-risk AI systems</code> include applications in critical sectors such as healthcare, education, or law enforcement. These applications face extensive regulations, including <code>risk management systems</code>, <code>data governance</code>, and <code>human oversight</code>.</li>
<li>
<code>Limited-risk AI systems</code> directly interact with people or generate content. This category includes LLMs. These systems mainly face obligations regarding <code>transparency</code> and <code>documentation</code> requirements. For instance, service providers are required to disclose if content is AI-generated and implement safeguards to prevent misuse, such as abuse attacks.</li>
<li>
<code>Minimal-risk AI systems</code>: These systems include spam filters or video games and are largely <code>unregulated</code>.</li>
</ul>
