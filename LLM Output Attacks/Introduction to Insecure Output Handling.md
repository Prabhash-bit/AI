<h1>Introduction to Insecure Output Handling</h1>
<hr />
<p>Many common security vulnerabilities arise from improper handling of untrusted data. Arguably the most common attack vector is an <code>Injection Attack</code>. Typical examples in the web domain include Cross-Site Scripting (XSS), where untrusted data is inserted into the HTML DOM, leading to the execution of arbitrary JavaScript code; SQL Injection, where untrusted data is inserted into SQL queries, leading to the execution of arbitrary SQL queries; and code injection, where untrusted data is inserted into system commands, leading to the execution of arbitrary system commands.</p>
<p>This module will only discuss output attacks against text-based models, i.e., LLMs. However, in real-world deployments it is common to interact with multimodal models that can process and generate text as well as images, audio, and video. These types of models provide additional attack surfaces for output attacks.</p>
<hr />
<h2>Insecure Output Handling in LLM Applications</h2>
<p>Text generated by <code>Large Language Models (LLMs)</code> needs to be treated as untrusted data since there is no direct control over the LLM's response. As such, the output must be subjected to the same types of validation, sanitization, and escaping that untrusted user input is subjected to. For instance, if an LLM's output is reflected in a web server's response in any endpoint, proper HTML encoding must be applied. Similarly, if we insert an LLM's output into a SQL query, we must apply preventive measures such as prepared statements or escaping.</p>
<p>However, insecure handling of LLM output can not only lead to injection vulnerabilities. For instance, if an LLM is used to generate an e-mail body, improper output validation may lead to malicious, illegal, or unethical content being contained in the e-mail. A company sending such an e-mail to a potential customer may suffer financial or reputational damage. Another source for potential security vulnerabilities is source code snippets generated by LLMs. If they are not adequately reviewed for bugs and security issues, vulnerabilities may unknowingly get introduced into code bases.</p>
<hr />
<h2>Recap: OWASP LLM Top 10</h2>
<p>Before diving into concrete attack techniques, let us take a moment and recap where security vulnerabilities discussed throughout this module are situated in OWASP's <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">Top 10 for LLM Applications</a>. As the module name suggests, we will explore attack techniques for <code>LLM05: 2025 Improper Output Handling</code>. As discussed above, this security risk refers to all instances where LLM output is not treated as untrusted data and proper sanitization, validation, or escaping is not applied. In <a href="https://saif.google/secure-ai-framework/risks">Google's SAIF</a>, the attack vectors discussed in this module fall under the <code>Insecure Model Output</code> risk.</p>
