Red Teaming ML-based Systems
Unlike traditional systems, ML-based systems face unique vulnerabilities because they rely on large datasets, statistical inference, and complex model architectures. 
Thus, red team assessments are often the way to go when assessing the security of ML-based systems, as many advanced attack techniques require more time than a typical penetration test would last. 
Furthermore, ML-based systems are comprised of various components that interact with each other. Often, security vulnerabilities arise at these interaction points. As such, including all these components in the security assessment is beneficial. 
Determining the scope of a penetration test for an ML-based system can be difficult. It may inadvertently exclude specific components or interaction points, potentially making particular security vulnerabilities impossible to reveal.

Attacking ML-based Systems (ML OWASP Top 10)
----------------------------------------------------------
Just like for Web Applications, Web APIs, and Mobile Applications, OWASP has published a Top 10 list of security risks regarding the deployment and management of ML-based Systems, the Top 10 for Machine Learning Security. 
We will briefly discuss the ten risks to obtain an overview of security issues resulting from ML-based systems.


ID	                                                                          Description
ML01	Input Manipulation Attack:                                              Attackers modify input data to cause incorrect or malicious model outputs.
ML02	Data Poisoning Attack:                                                  Attackers inject malicious or misleading data into training data, compromising model performance or creating backdoors.
ML03	Model Inversion Attack:                                                 Attackers train a separate model to reconstruct inputs from model outputs, potentially revealing sensitive information.
ML04	Membership Inference Attack:                                            Attackers analyze model behavior to determine whether data was included in the model's training data set, potentially revealing sensitive information.
ML05	Model Theft:                                                            Attackers train a separate model from interactions with the original model, thereby stealing intellectual property.
ML06	AI Supply Chain Attacks:                                                Attackers exploit vulnerabilities in any part of the ML supply chain.
ML07	Transfer Learning Attack:                                               Attackers manipulate the baseline model that is subsequently fine-tuned by a third-party. This can lead to biased or backdoored models.
ML08	Model Skewing:                                                          Attackers skew the model's behavior for malicious purposes, for instance, by manipulating the training data set.
ML09	Output Integrity Attack:                                                Attackers manipulate a model's output before processing, making it look like the model produced a different output.
ML10	Model Poisoning:                                                        Attackers manipulate the model's weights, compromising model performance or creating backdoors.



Input Manipulation Attack (ML01)
As the name suggests, input manipulation attacks comprise any type of attack against an ML model that results from manipulating the input data. 
Typically, the result of these attacks is unexpected behavior of the ML model that deviates from the intended behavior. The impact depends highly on the concrete scenario and circumstances in which the model is used. 
It can range from financial and reputational damage to legal consequences or data loss.

Many real-world input manipulation attack vectors apply small perturbations to benign input data, resulting in unexpected behavior by the ML model. In contrast, the perturbations are so small that the input looks
benign to the human eye. For instance, consider a self-driving car that uses an ML-based system for image classification of road signs to detect the current speed limit, stop signs, etc. In an input manipulation attack, 
an attacker could add small perturbations like particularly placed dirt specks, small stickers, or graffiti to road signs. While these perturbations look harmless to the human eye, they could result
in the misclassification of the sign by the ML-based system. This can have deadly consequences for passengers of the vehicle. For more details on this attack vector, check out this and this paper.


https://arxiv.org/pdf/1707.08945
https://arxiv.org/pdf/2307.08278



Data Poisoning Attack (ML02)
---------------------------------------
Data poisoning attacks on ML-based systems involve injecting malicious or misleading data into the training dataset to compromise the model's accuracy, performance, or behavior. 
As discussed before, the quality of any ML model is highly dependent on the quality of the training data. As such, these attacks can cause a model to make incorrect predictions, misclassify certain inputs, 
or behave unpredictably in specific scenarios. ML models often rely on large-scale, automated data collection from various sources, so they may be more susceptible to such tampering, 
especially when the sources are unverified or gathered from public domains.

As an example, assume an adversary is able to inject malicious data into the training data set for a model used in antivirus software to decide whether a given binary is malware. 
The adversary may manipulate the training data to effectively establish a backdoor that enables them to create custom malware, which the model classifies as a benign binary. 
More details about installing backdoors through data poisoning attacks are discussed in this paper.

https://arxiv.org/pdf/2408.13221
























