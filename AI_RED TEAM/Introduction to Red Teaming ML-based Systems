Red Teaming ML-based Systems
Unlike traditional systems, ML-based systems face unique vulnerabilities because they rely on large datasets, statistical inference, and complex model architectures. 
Thus, red team assessments are often the way to go when assessing the security of ML-based systems, as many advanced attack techniques require more time than a typical penetration test would last. 
Furthermore, ML-based systems are comprised of various components that interact with each other. Often, security vulnerabilities arise at these interaction points. As such, including all these components in the security assessment is beneficial. 
Determining the scope of a penetration test for an ML-based system can be difficult. It may inadvertently exclude specific components or interaction points, potentially making particular security vulnerabilities impossible to reveal.

Attacking ML-based Systems (ML OWASP Top 10)
----------------------------------------------------------
Just like for Web Applications, Web APIs, and Mobile Applications, OWASP has published a Top 10 list of security risks regarding the deployment and management of ML-based Systems, the Top 10 for Machine Learning Security. 
We will briefly discuss the ten risks to obtain an overview of security issues resulting from ML-based systems.


ID	                                                                          Description
ML01	Input Manipulation Attack:                                              Attackers modify input data to cause incorrect or malicious model outputs.
ML02	Data Poisoning Attack:                                                  Attackers inject malicious or misleading data into training data, compromising model performance or creating backdoors.
ML03	Model Inversion Attack:                                                 Attackers train a separate model to reconstruct inputs from model outputs, potentially revealing sensitive information.
ML04	Membership Inference Attack:                                            Attackers analyze model behavior to determine whether data was included in the model's training data set, potentially revealing sensitive information.
ML05	Model Theft:                                                            Attackers train a separate model from interactions with the original model, thereby stealing intellectual property.
ML06	AI Supply Chain Attacks:                                                Attackers exploit vulnerabilities in any part of the ML supply chain.
ML07	Transfer Learning Attack:                                               Attackers manipulate the baseline model that is subsequently fine-tuned by a third-party. This can lead to biased or backdoored models.
ML08	Model Skewing:                                                          Attackers skew the model's behavior for malicious purposes, for instance, by manipulating the training data set.
ML09	Output Integrity Attack:                                                Attackers manipulate a model's output before processing, making it look like the model produced a different output.
ML10	Model Poisoning:                                                        Attackers manipulate the model's weights, compromising model performance or creating backdoors.



Input Manipulation Attack (ML01)
As the name suggests, input manipulation attacks comprise any type of attack against an ML model that results from manipulating the input data. 
Typically, the result of these attacks is unexpected behavior of the ML model that deviates from the intended behavior. The impact depends highly on the concrete scenario and circumstances in which the model is used. 
It can range from financial and reputational damage to legal consequences or data loss.

Many real-world input manipulation attack vectors apply small perturbations to benign input data, resulting in unexpected behavior by the ML model. In contrast, the perturbations are so small that the input looks
benign to the human eye. For instance, consider a self-driving car that uses an ML-based system for image classification of road signs to detect the current speed limit, stop signs, etc. In an input manipulation attack, 
an attacker could add small perturbations like particularly placed dirt specks, small stickers, or graffiti to road signs. While these perturbations look harmless to the human eye, they could result
in the misclassification of the sign by the ML-based system. This can have deadly consequences for passengers of the vehicle. For more details on this attack vector, check out this and this paper.


https://arxiv.org/pdf/1707.08945
https://arxiv.org/pdf/2307.08278



Data Poisoning Attack (ML02)
---------------------------------------
Data poisoning attacks on ML-based systems involve injecting malicious or misleading data into the training dataset to compromise the model's accuracy, performance, or behavior. 
As discussed before, the quality of any ML model is highly dependent on the quality of the training data. As such, these attacks can cause a model to make incorrect predictions, misclassify certain inputs, 
or behave unpredictably in specific scenarios. ML models often rely on large-scale, automated data collection from various sources, so they may be more susceptible to such tampering, 
especially when the sources are unverified or gathered from public domains.

As an example, assume an adversary is able to inject malicious data into the training data set for a model used in antivirus software to decide whether a given binary is malware. 
The adversary may manipulate the training data to effectively establish a backdoor that enables them to create custom malware, which the model classifies as a benign binary. 
More details about installing backdoors through data poisoning attacks are discussed in this paper.

https://arxiv.org/pdf/2408.13221


Storage
---------------------------------------------
Following collection, data requires storage. The choice of technology hinges on the data's structure, volume, and access patterns. Structured data often resides in relational databases (PostgreSQL), 
while semi-structured logs might use NoSQL databases (MongoDB). For large, diverse datasets, organizations frequently employ data lakes built on distributed file systems (Hadoop HDFS) or cloud object storage (AWS S3, 
Azure Blob Storage). Specialized databases like InfluxDB cater to time-series data. Importantly, trained models themselves become stored artifacts, often serialized into formats like Python's pickle (.pkl), ONNX, or 
framework-specific files (.pt, .pth, .safetensors), each presenting unique security considerations if handled improperly.


Data Processing
---------------------------
Next, raw data undergoes data processing and transformation, as it's rarely suitable for direct model use. This stage employs various libraries and frameworks for cleaning, normalization, and 
feature engineering. Data cleaning might involve handling missing values using Pandas and scikit-learn's Imputers. Feature scaling often uses StandardScaler or MinMaxScaler. Feature engineering creates new relevant inputs, 
such as extracting date components or, for text data, performing tokenization and embedding generation using NLTK or spaCy. Image data might be augmented using OpenCV or Pillow. 
Large datasets often necessitate distributed processing frameworks like Apache Spark or Dask, with orchestration tools like Apache Airflow or Kubeflow Pipelines managing these complex workflows. 
The objective is to prepare a high-quality dataset optimized for the AI task.

Modeling
----------------------------------------
The processed data then fuels the analysis and modeling stage. Data scientists and ML engineers explore the data, often within interactive environments like Jupyter Notebooks, and train models using frameworks 
such as scikit-learn, TensorFlow, Jax, or PyTorch. This iterative process involves selecting algorithms (e.g., RandomForestClassifier, CNNs), tuning hyperparameters (perhaps using Optuna), and validating performance. 
Cloud platforms like AWS SageMaker or Azure Machine Learning often provide integrated environments for this lifecycle.


Deployment
---------------------------
Once trained and validated, a model enters the deployment stage, where it's integrated into a production environment to serve predictions. 
Common patterns include exposing the model as a REST API using frameworks like Flask or FastAPI, often containerized with Docker and orchestrated by Kubernetes. Alternatively, 
models might become serverless functions (AWS Lambda) or be embedded directly into applications or edge devices (using formats like TensorFlow Lite). 
Securing the deployed model file and its surrounding infrastructure is a key concern here.


Monitoring and Maintenance
-----------------------------------------
Finally, monitoring and maintenance constitute an ongoing stage. Deployed models are continuously observed for operational health using tools like Prometheus and Grafana, 
while specialized ML monitoring platforms (WhyLabs, Arize AI) track data drift, concept drift, and prediction quality. 
Feedback from predictions and user interactions is logged and often processed alongside newly collected data to periodically retrain the model. This retraining is essential for adapting to changing patterns and 
maintaining performance but simultaneously creates a significant attack vector. Malicious data introduced via feedback loops or ongoing collection can be incorporated during retraining, enabling online poisoning attacks. 
Orchestration tools like Airflow often manage these retraining pipelines, making the security of data flowing into them critical.


Two Pipeline Examples
To clearly illustrate these complex pipelines, lets consider two examples:

First, an e-commerce platform building a product recommendation system collects user activity (JSON logs via Kafka) and reviews (text). This raw data lands in a data lake (AWS S3). 
Apache Spark processes this data, reconstructing sessions and performing sentiment analysis (NLTK) on reviews, outputting Parquet files. Within AWS SageMaker, a recommendation model is trained on this processed data. 
The resulting model file (pickle format) is stored back in S3 before being deployed via a Docker-ized Flask API on Kubernetes.
Monitoring tracks click-through rates, and user feedback along with new interaction data feeds into periodic retraining cycles managed by Airflow, aiming to keep recommendations 
relevant but also opening the door for potential poisoning through manipulated feedback.

Second, a healthcare provider developing a predictive diagnostic tool collects anonymized patient images (DICOM) and notes (XML) from PACS and EHR systems.
Secure storage (e.g., HIPAA-compliant AWS S3) is a requirement here. Python scripts using Pydicom, OpenCV, and spaCy process the data, standardizing images and extracting features. 
PyTorch trains a deep learning model (CNN) on specialized hardware. The validated model (.pt file) is securely stored and then deployed via an internal API to a clinical decision support system.
Monitoring tracks diagnostic accuracy and data drift. While retraining might be less frequent and more rigorously controlled here, 
incorporating new data or corrected diagnoses still requires careful validation to prevent poisoning.











